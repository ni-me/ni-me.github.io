<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Coursera-机器学习-吴恩达-ex4 | CodeTrainer</title><meta name="description" content="Coursera-机器学习-吴恩达-ex4"><meta name="keywords" content="机器学习"><meta name="author" content="大巴斯基"><meta name="copyright" content="大巴斯基"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon1.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Coursera-机器学习-吴恩达-ex4"><meta name="twitter:description" content="Coursera-机器学习-吴恩达-ex4"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta property="og:type" content="article"><meta property="og:title" content="Coursera-机器学习-吴恩达-ex4"><meta property="og:url" content="http://nieblog.me/2020/01/14/ml_ex4/"><meta property="og:site_name" content="CodeTrainer"><meta property="og:description" content="Coursera-机器学习-吴恩达-ex4"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://nieblog.me/2020/01/14/ml_ex4/"><link rel="prev" title="Coursera-机器学习-吴恩达-ex3" href="http://nieblog.me/2020/01/14/ml_ex3/"><link rel="next" title="Coursera-机器学习-吴恩达-ex2" href="http://nieblog.me/2020/01/14/ml_ex2/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">CodeTrainer</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">21</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">6</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#sigmoidgradientm"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> sigmoidGradient.m</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#randinitializeweightsm"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> randInitializeWeights.m</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#nncostfunctionm"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text"> nnCostFunction.m</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#sigmoidgradientm"><span class="toc-number">1.</span> <span class="toc-text"> sigmoidGradient.m</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#randinitializeweightsm"><span class="toc-number">2.</span> <span class="toc-text"> randInitializeWeights.m</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nncostfunctionm"><span class="toc-number">3.</span> <span class="toc-text"> nnCostFunction.m</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png)"><div id="post-info"><div id="post-title"><div class="posttitle">Coursera-机器学习-吴恩达-ex4</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-01-14<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-01-15</time><div class="post-meta-wordcount"><div class="post-meta-pv-cv"></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p><a href="https://github.com/ni-me/ML_programming_exercise/tree/master/ex/machine-learning-ex4" target="_blank" rel="noopener">代码和作业说明下载</a><br>
这次作业我们需要实现 Neural Networks Learning。</p>
<p>需要完成下列代码文件：</p>
<ul>
<li>sigmoidGradient.m</li>
<li>randInitializeWeights.m</li>
<li>nnCostFunction.m</li>
</ul>
<h2 id="sigmoidgradientm"><a class="markdownIt-Anchor" href="#sigmoidgradientm"></a> sigmoidGradient.m</h2>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">matlab</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">g</span> = <span class="title">sigmoidGradient</span><span class="params">(z)</span></span></div><div class="line"><span class="comment">%SIGMOIDGRADIENT returns the gradient of the sigmoid function</span></div><div class="line"><span class="comment">%evaluated at z</span></div><div class="line"><span class="comment">%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function</span></div><div class="line"><span class="comment">%   evaluated at z. This should work regardless if z is a matrix or a</span></div><div class="line"><span class="comment">%   vector. In particular, if z is a vector or matrix, you should return</span></div><div class="line"><span class="comment">%   the gradient for each element.</span></div><div class="line"></div><div class="line">g = <span class="built_in">zeros</span>(<span class="built_in">size</span>(z));</div><div class="line"></div><div class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></div><div class="line"><span class="comment">% Instructions: Compute the gradient of the sigmoid function evaluated at</span></div><div class="line"><span class="comment">%               each value of z (z can be a matrix, vector or scalar).</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">g = sigmoid(z) .* (<span class="number">1</span> - sigmoid(z));</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">% =============================================================</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></div>
<h2 id="randinitializeweightsm"><a class="markdownIt-Anchor" href="#randinitializeweightsm"></a> randInitializeWeights.m</h2>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">matlab</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">W</span> = <span class="title">randInitializeWeights</span><span class="params">(L_in, L_out)</span></span></div><div class="line"><span class="comment">%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in</span></div><div class="line"><span class="comment">%incoming connections and L_out outgoing connections</span></div><div class="line"><span class="comment">%   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights </span></div><div class="line"><span class="comment">%   of a layer with L_in incoming connections and L_out outgoing </span></div><div class="line"><span class="comment">%   connections. </span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%   Note that W should be set to a matrix of size(L_out, 1 + L_in) as</span></div><div class="line"><span class="comment">%   the first column of W handles the "bias" terms</span></div><div class="line"><span class="comment">%</span></div><div class="line"></div><div class="line"><span class="comment">% You need to return the following variables correctly </span></div><div class="line">W = <span class="built_in">zeros</span>(L_out, <span class="number">1</span> + L_in);</div><div class="line"></div><div class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></div><div class="line"><span class="comment">% Instructions: Initialize W randomly so that we break the symmetry while</span></div><div class="line"><span class="comment">%               training the neural network.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% Note: The first column of W corresponds to the parameters for the bias unit</span></div><div class="line"><span class="comment">%</span></div><div class="line"></div><div class="line">epsilon_init  = <span class="number">0.12</span>;</div><div class="line">W = <span class="built_in">rand</span>(L_out, <span class="number">1</span> + L_in) * <span class="number">2</span> * epsilon_init - epsilon_init;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">% =========================================================================</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></div>
<h2 id="nncostfunctionm"><a class="markdownIt-Anchor" href="#nncostfunctionm"></a> nnCostFunction.m</h2>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">matlab</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J grad]</span> = <span class="title">nnCostFunction</span><span class="params">(nn_params, ...</span></span></div><div class="line"><span class="function"><span class="params">                                   input_layer_size, ...</span></span></div><div class="line"><span class="function"><span class="params">                                   hidden_layer_size, ...</span></span></div><div class="line"><span class="function"><span class="params">                                   num_labels, ...</span></span></div><div class="line"><span class="function"><span class="params">                                   X, y, lambda)</span></span></div><div class="line"><span class="comment">%NNCOSTFUNCTION Implements the neural network cost function for a two layer</span></div><div class="line"><span class="comment">%neural network which performs classification</span></div><div class="line"><span class="comment">%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...</span></div><div class="line"><span class="comment">%   X, y, lambda) computes the cost and gradient of the neural network. The</span></div><div class="line"><span class="comment">%   parameters for the neural network are "unrolled" into the vector</span></div><div class="line"><span class="comment">%   nn_params and need to be converted back into the weight matrices. </span></div><div class="line"><span class="comment">% </span></div><div class="line"><span class="comment">%   The returned parameter grad should be a "unrolled" vector of the</span></div><div class="line"><span class="comment">%   partial derivatives of the neural network.</span></div><div class="line"><span class="comment">%</span></div><div class="line"></div><div class="line"><span class="comment">% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices</span></div><div class="line"><span class="comment">% for our 2 layer neural network</span></div><div class="line">Theta1 = <span class="built_in">reshape</span>(nn_params(<span class="number">1</span>:hidden_layer_size * (input_layer_size + <span class="number">1</span>)), ...</div><div class="line">                 hidden_layer_size, (input_layer_size + <span class="number">1</span>));</div><div class="line"></div><div class="line">Theta2 = <span class="built_in">reshape</span>(nn_params((<span class="number">1</span> + (hidden_layer_size * (input_layer_size + <span class="number">1</span>))):<span class="keyword">end</span>), ...</div><div class="line">                 num_labels, (hidden_layer_size + <span class="number">1</span>));</div><div class="line"></div><div class="line"><span class="comment">% Setup some useful variables</span></div><div class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</div><div class="line">         </div><div class="line"><span class="comment">% You need to return the following variables correctly </span></div><div class="line">J = <span class="number">0</span>;</div><div class="line">Theta1_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta1));</div><div class="line">Theta2_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta2));</div><div class="line"></div><div class="line"><span class="comment">% ====================== YOUR CODE HERE ======================</span></div><div class="line"><span class="comment">% Instructions: You should complete the code by working through the</span></div><div class="line"><span class="comment">%               following parts.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% Part 1: Feedforward the neural network and return the cost in the</span></div><div class="line"><span class="comment">%         variable J. After implementing Part 1, you can verify that your</span></div><div class="line"><span class="comment">%         cost function computation is correct by verifying the cost</span></div><div class="line"><span class="comment">%         computed in ex4.m</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% Part 2: Implement the backpropagation algorithm to compute the gradients</span></div><div class="line"><span class="comment">%         Theta1_grad and Theta2_grad. You should return the partial derivatives of</span></div><div class="line"><span class="comment">%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and</span></div><div class="line"><span class="comment">%         Theta2_grad, respectively. After implementing Part 2, you can check</span></div><div class="line"><span class="comment">%         that your implementation is correct by running checkNNGradients</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%         Note: The vector y passed into the function is a vector of labels</span></div><div class="line"><span class="comment">%               containing values from 1..K. You need to map this vector into a </span></div><div class="line"><span class="comment">%               binary vector of 1's and 0's to be used with the neural network</span></div><div class="line"><span class="comment">%               cost function.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%         Hint: We recommend implementing backpropagation using a for-loop</span></div><div class="line"><span class="comment">%               over the training examples if you are implementing it for the </span></div><div class="line"><span class="comment">%               first time.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">% Part 3: Implement regularization with the cost function and gradients.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%         Hint: You can implement this around the code for</span></div><div class="line"><span class="comment">%               backpropagation. That is, you can compute the gradients for</span></div><div class="line"><span class="comment">%               the regularization separately and then add them to Theta1_grad</span></div><div class="line"><span class="comment">%               and Theta2_grad from Part 2.</span></div><div class="line"><span class="comment">%</span></div><div class="line"><span class="comment">%</span></div><div class="line"></div><div class="line">a1 = [ones(m, <span class="number">1</span>) X];</div><div class="line"></div><div class="line">Y_tmp = <span class="built_in">zeros</span>(m, num_labels);</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  Y_tmp(<span class="built_in">i</span>, y(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">a2 = [ones(m, <span class="number">1</span>) sigmoid(a1 * (Theta1<span class="string">'))];</span></div><div class="line"><span class="string">h = sigmoid(a2 * (Theta2'</span>));</div><div class="line"></div><div class="line">J = (<span class="number">-1</span> / m) * sum(sum((Y_tmp .* log(h) + (<span class="number">1</span> - Y_tmp) .* log(<span class="number">1</span> - h))<span class="string">'));</span></div><div class="line"><span class="string">%--------------------------------------------------------------------------</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Theta1_tmp = Theta1(:, 2:end);</span></div><div class="line"><span class="string">Theta2_tmp = Theta2(:, 2:end);</span></div><div class="line"><span class="string">regular_term = (lambda / (2 * m)) * (sum(sum((Theta1_tmp .^ 2)'</span>)) + ...</div><div class="line">                  sum(sum((Theta2_tmp .^ <span class="number">2</span>)<span class="string">')));</span></div><div class="line"><span class="string">J = J + regular_term;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">%--------------------------------------------------------------------------</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Yk = zeros(m, num_labels);</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">for i = 1 : m</span></div><div class="line"><span class="string">  Yk(i, y(i)) = 1;</span></div><div class="line"><span class="string">end</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">%&#123;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">for i = 1 : m</span></div><div class="line"><span class="string">  a1 = [1, X(i, :)];  % 1x401</span></div><div class="line"><span class="string">  z2 = a1 * Theta1'</span>;</div><div class="line">  a2 = sigmoid(z2);</div><div class="line">  a2 = [<span class="number">1</span>, a2];       <span class="comment">% 1x26</span></div><div class="line">  z3 = a2 * Theta2';</div><div class="line">  a3 = sigmoid(z3);</div><div class="line">  delta3 = a3 - Yk(<span class="built_in">i</span>, :); <span class="comment">% 1x10</span></div><div class="line">  delta2 = delta3 * Theta2(:, <span class="number">2</span>:<span class="keyword">end</span>) .* sigmoidGradient(z2); <span class="comment">% 1x25</span></div><div class="line">  </div><div class="line">  Theta1_grad = Theta1_grad + delta2' * a1;</div><div class="line">  Theta2_grad = Theta2_grad + delta3' * a2;</div><div class="line">  </div><div class="line">endfor</div><div class="line"></div><div class="line"><span class="comment">%&#125;</span></div><div class="line"></div><div class="line">a1 = [ones(m, <span class="number">1</span>) X];  <span class="comment">% 5000 x 401</span></div><div class="line">z2 = a1 * Theta1';    <span class="comment">% 5000 x 401  401 x 25 -&gt; 5000 x 25</span></div><div class="line">a2 = [ones(m, <span class="number">1</span>) sigmoid(z2)];     <span class="comment">% 5000 x 26</span></div><div class="line">z3 = a2 * Theta2';    <span class="comment">% 5000 x 26   26 x 10  -&gt; 5000 - 10</span></div><div class="line">a3 = sigmoid(z3);     <span class="comment">% 5000 x 10</span></div><div class="line">delta3 = a3 - Yk;     <span class="comment">% 5000 x 10</span></div><div class="line">delta2 = delta3 * Theta2(:, <span class="number">2</span>:<span class="keyword">end</span>) .* sigmoidGradient(z2); <span class="comment">% 5000 x 10  10 x 25 -&gt; 5000 x 25</span></div><div class="line"></div><div class="line">Theta1_grad = Theta1_grad + delta2' * a1;           <span class="comment">% 25 x 5000  5000 x 401   -&gt; 25 x 401</span></div><div class="line">Theta2_grad = Theta2_grad + delta3' * a2;           <span class="comment">% 10 x 5000  5000 x 26    -&gt; 10 x 26</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Theta1_grad = Theta1_grad ./ m;</div><div class="line">Theta2_grad = Theta2_grad ./ m;</div><div class="line"><span class="comment">%----------------------------------------------------------------</span></div><div class="line">Theta1_tmp = Theta1;</div><div class="line">Theta2_tmp = Theta2;</div><div class="line">Theta1_tmp(:, <span class="number">1</span>) = <span class="number">0</span>;</div><div class="line">Theta2_tmp(:, <span class="number">1</span>) = <span class="number">0</span>;</div><div class="line"></div><div class="line">Theta1_grad = Theta1_grad + (lambda / m) .* Theta1_tmp;</div><div class="line"></div><div class="line">Theta2_grad = Theta2_grad + (lambda / m) .* Theta2_tmp;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">% =========================================================================</span></div><div class="line"></div><div class="line"><span class="comment">% Unroll gradients</span></div><div class="line">grad = [Theta1_grad(:) ; Theta2_grad(:)];</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></div></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">大巴斯基</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://nieblog.me/2020/01/14/ml_ex4/">http://nieblog.me/2020/01/14/ml_ex4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://nieblog.me">CodeTrainer</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/机器学习/">机器学习    </a></div><div class="post_share"></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/01/14/ml_ex3/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>Coursera-机器学习-吴恩达-ex3</span></div></a></div><div class="next-post pull_right"><a href="/2020/01/14/ml_ex2/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Coursera-机器学习-吴恩达-ex2</span></div></a></div></nav></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2017 - 2020 By 大巴斯基</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>